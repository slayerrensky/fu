[{"url":"https://api.github.com/repos/owncloud/core/issues/comments/10552933","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-10552933","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":10552933,"user":{"login":"blizzz","id":2184312,"avatar_url":"https://gravatar.com/avatar/1898b25a9cb3aa1a0f0febd1359910b9?d=https%3a%2f%2fidenticons.github.com%2fca03acec6deadf508b75eb0cf9ba036c.png&r=x","gravatar_id":"1898b25a9cb3aa1a0f0febd1359910b9","url":"https://api.github.com/users/blizzz","html_url":"https://github.com/blizzz","followers_url":"https://api.github.com/users/blizzz/followers","following_url":"https://api.github.com/users/blizzz/following{/other_user}","gists_url":"https://api.github.com/users/blizzz/gists{/gist_id}","starred_url":"https://api.github.com/users/blizzz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/blizzz/subscriptions","organizations_url":"https://api.github.com/users/blizzz/orgs","repos_url":"https://api.github.com/users/blizzz/repos","events_url":"https://api.github.com/users/blizzz/events{/privacy}","received_events_url":"https://api.github.com/users/blizzz/received_events","type":"user","site_admin":false},"created_at":"2012-11-20t12:55:42z","updated_at":"2012-11-20t12:55:42z","body":"which client version?"},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/10553948","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-10553948","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":10553948,"user":{"login":"mpfj","id":1272389,"avatar_url":"https://gravatar.com/avatar/01bbb7dfb0fea08d9dc6b0700e55c48d?d=https%3a%2f%2fidenticons.github.com%2fd51d1caeda8b2bb43df001997711fbe4.png&r=x","gravatar_id":"01bbb7dfb0fea08d9dc6b0700e55c48d","url":"https://api.github.com/users/mpfj","html_url":"https://github.com/mpfj","followers_url":"https://api.github.com/users/mpfj/followers","following_url":"https://api.github.com/users/mpfj/following{/other_user}","gists_url":"https://api.github.com/users/mpfj/gists{/gist_id}","starred_url":"https://api.github.com/users/mpfj/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mpfj/subscriptions","organizations_url":"https://api.github.com/users/mpfj/orgs","repos_url":"https://api.github.com/users/mpfj/repos","events_url":"https://api.github.com/users/mpfj/events{/privacy}","received_events_url":"https://api.github.com/users/mpfj/received_events","type":"user","site_admin":false},"created_at":"2012-11-20t13:32:31z","updated_at":"2012-11-20t13:32:31z","body":"latest client for windows (1.1.1)"},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/10691070","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-10691070","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":10691070,"user":{"login":"deepdiver1975","id":1005065,"avatar_url":"https://gravatar.com/avatar/bdba071ed68da9ce8edae53c364902ef?d=https%3a%2f%2fidenticons.github.com%2f67e101ad9531b7d25697b76e503e062e.png&r=x","gravatar_id":"bdba071ed68da9ce8edae53c364902ef","url":"https://api.github.com/users/deepdiver1975","html_url":"https://github.com/deepdiver1975","followers_url":"https://api.github.com/users/deepdiver1975/followers","following_url":"https://api.github.com/users/deepdiver1975/following{/other_user}","gists_url":"https://api.github.com/users/deepdiver1975/gists{/gist_id}","starred_url":"https://api.github.com/users/deepdiver1975/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/deepdiver1975/subscriptions","organizations_url":"https://api.github.com/users/deepdiver1975/orgs","repos_url":"https://api.github.com/users/deepdiver1975/repos","events_url":"https://api.github.com/users/deepdiver1975/events{/privacy}","received_events_url":"https://api.github.com/users/deepdiver1975/received_events","type":"user","site_admin":false},"created_at":"2012-11-25t07:16:59z","updated_at":"2012-11-25t07:16:59z","body":"please reopen this issue within the mirall repo. thx"},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/17272325","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-17272325","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":17272325,"user":{"login":"danimo","id":1315934,"avatar_url":"https://gravatar.com/avatar/9467db6bbeaac42484603e0788aec39d?d=https%3a%2f%2fidenticons.github.com%2f296ab09e5a31dcf357f4d4784453db99.png&r=x","gravatar_id":"9467db6bbeaac42484603e0788aec39d","url":"https://api.github.com/users/danimo","html_url":"https://github.com/danimo","followers_url":"https://api.github.com/users/danimo/followers","following_url":"https://api.github.com/users/danimo/following{/other_user}","gists_url":"https://api.github.com/users/danimo/gists{/gist_id}","starred_url":"https://api.github.com/users/danimo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danimo/subscriptions","organizations_url":"https://api.github.com/users/danimo/orgs","repos_url":"https://api.github.com/users/danimo/repos","events_url":"https://api.github.com/users/danimo/events{/privacy}","received_events_url":"https://api.github.com/users/danimo/received_events","type":"user","site_admin":false},"created_at":"2013-05-01t08:04:34z","updated_at":"2013-05-01t08:04:34z","body":"i am reopening this issue here, since without the server providing an md5 sum, there is nothing we can do in the client."},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/17272339","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-17272339","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":17272339,"user":{"login":"danimo","id":1315934,"avatar_url":"https://gravatar.com/avatar/9467db6bbeaac42484603e0788aec39d?d=https%3a%2f%2fidenticons.github.com%2f296ab09e5a31dcf357f4d4784453db99.png&r=x","gravatar_id":"9467db6bbeaac42484603e0788aec39d","url":"https://api.github.com/users/danimo","html_url":"https://github.com/danimo","followers_url":"https://api.github.com/users/danimo/followers","following_url":"https://api.github.com/users/danimo/following{/other_user}","gists_url":"https://api.github.com/users/danimo/gists{/gist_id}","starred_url":"https://api.github.com/users/danimo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danimo/subscriptions","organizations_url":"https://api.github.com/users/danimo/orgs","repos_url":"https://api.github.com/users/danimo/repos","events_url":"https://api.github.com/users/danimo/events{/privacy}","received_events_url":"https://api.github.com/users/danimo/received_events","type":"user","site_admin":false},"created_at":"2013-05-01t08:05:17z","updated_at":"2013-05-01t08:05:17z","body":"note: we could at least have hash sums for the files that we have exclusive access to."},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/22329510","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-22329510","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":22329510,"user":{"login":"jancborchardt","id":925062,"avatar_url":"https://gravatar.com/avatar/2fd3f4d5d762955e5b603794a888fa97?d=https%3a%2f%2fidenticons.github.com%2f84ec51f9ceed468217d4e13abf78da9c.png&r=x","gravatar_id":"2fd3f4d5d762955e5b603794a888fa97","url":"https://api.github.com/users/jancborchardt","html_url":"https://github.com/jancborchardt","followers_url":"https://api.github.com/users/jancborchardt/followers","following_url":"https://api.github.com/users/jancborchardt/following{/other_user}","gists_url":"https://api.github.com/users/jancborchardt/gists{/gist_id}","starred_url":"https://api.github.com/users/jancborchardt/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jancborchardt/subscriptions","organizations_url":"https://api.github.com/users/jancborchardt/orgs","repos_url":"https://api.github.com/users/jancborchardt/repos","events_url":"https://api.github.com/users/jancborchardt/events{/privacy}","received_events_url":"https://api.github.com/users/jancborchardt/received_events","type":"user","site_admin":false},"created_at":"2013-08-08t15:00:13z","updated_at":"2013-08-08t15:00:13z","body":"so whatâ€™s the call on this one? fixed? important to work on? please advise."},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/23879139","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-23879139","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":23879139,"user":{"login":"zatricky","id":3970906,"avatar_url":"https://gravatar.com/avatar/8f1abbebfccca89beabcc58bda7887c1?d=https%3a%2f%2fidenticons.github.com%2f704290d64eb633787cca98b863c490c5.png&r=x","gravatar_id":"8f1abbebfccca89beabcc58bda7887c1","url":"https://api.github.com/users/zatricky","html_url":"https://github.com/zatricky","followers_url":"https://api.github.com/users/zatricky/followers","following_url":"https://api.github.com/users/zatricky/following{/other_user}","gists_url":"https://api.github.com/users/zatricky/gists{/gist_id}","starred_url":"https://api.github.com/users/zatricky/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zatricky/subscriptions","organizations_url":"https://api.github.com/users/zatricky/orgs","repos_url":"https://api.github.com/users/zatricky/repos","events_url":"https://api.github.com/users/zatricky/events{/privacy}","received_events_url":"https://api.github.com/users/zatricky/received_events","type":"user","site_admin":false},"created_at":"2013-09-05t16:04:22z","updated_at":"2013-09-05t16:06:38z","body":"i'd say this is very important. can we get appropriate labels put on this issue?\r\n\r\ni had a similar issue where my server and my desktop/laptop etc are using dropbox. i want to have all that moved over to oc - but its a little crazy that the client will want to re-upload and/or re-download everything. its much cheaper (bandwidth and time-wise) to move everything over on all three platforms and have them all simply notice \"oh, everything is okay, nothing to do here\".\r\n\r\nideally, the checksum should actually be an indexed value in the database - in fact it should probably even be the primary key used to identify content. i believe that the system already supports \"move\" operations (moving a file to another folder within owncloud without causing a deletion/re-upload) - but doing this would actually make supporting this concept trivial.\r\n\r\nplease note, md5sum is a good starting point - but it would be much more appropriate to use a variety of checks and to use various cryptographic checksums to ensure that everything is consistent across all systems:ç‚\r\ntimestamp\r\nfilesize\r\nchecksum\r\n\r\nif filesize and checksum matches but timestamp differs, then only a tiny change should be actioned (fixing the timestamp).\r\nin any other case, an rsync-style differential upload/download should be actioned.\r\n\r\nç‚ all of this information should be stored in the client and server databases when files/folders are added to the repository. recalculating hash values every time the client starts would be madness. having this stored within the client database would also improve the \"time to first sync\" in the case of content having changed while the client wasn't running."},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/24322717","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-24322717","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":24322717,"user":{"login":"etiess","id":900280,"avatar_url":"https://gravatar.com/avatar/a16a4ae7279e14f0c608a8e6e9599276?d=https%3a%2f%2fidenticons.github.com%2f2544b8d9a1b6b72b456adc440616c148.png&r=x","gravatar_id":"a16a4ae7279e14f0c608a8e6e9599276","url":"https://api.github.com/users/etiess","html_url":"https://github.com/etiess","followers_url":"https://api.github.com/users/etiess/followers","following_url":"https://api.github.com/users/etiess/following{/other_user}","gists_url":"https://api.github.com/users/etiess/gists{/gist_id}","starred_url":"https://api.github.com/users/etiess/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/etiess/subscriptions","organizations_url":"https://api.github.com/users/etiess/orgs","repos_url":"https://api.github.com/users/etiess/repos","events_url":"https://api.github.com/users/etiess/events{/privacy}","received_events_url":"https://api.github.com/users/etiess/received_events","type":"user","site_admin":false},"created_at":"2013-09-12t14:14:14z","updated_at":"2013-09-12t14:14:14z","body":"hello,\r\n\r\nfirst of all, thanks to the dev for the amazing owncloud.\r\n\r\ni would like to know how far you are with solving this type of issue?\r\n\r\ni live and work in africa: internet access are slow, and most of the time data plans are limited in download. so itâ€™s very expensive to resync a folder on a new computer with some gb of data. with dropbox, we just copy the files on an usb stick, and copy them to the new computer before syncronizing it. then we donâ€™t need to download data again and the sync works fine.\r\n\r\nhow is it possible with owncloud? does the unique id make this possible? when i copy my files, can i copy the unique id too? if not, we are forced to download again, what is very unconfortable for large amount of dataâ€¦\r\n\r\ni tried to copy the whole folder, but it doesn't seem to work:\r\n- i copy the entire folder locally, with a new name\r\n- i set up a new folder sync with mirall (1.4.0 on windows 8 / owncloud 5.0.11 on ubuntu server 13.04)\r\n- i launch the sync: it downloads everything again for the new sync :-( and it even created conflicted copies of each files :-( ( (and conflict files don't have the same size, see attached picture)\r\n![capture](https://f.cloud.github.com/assets/900280/1131095/fcbba268-1bb4-11e3-8bbf-7e90d9d2ed86.jpg)\r\n\r\n\r\ni have to precise that i do not use owncloud in a standard way: i donâ€™t want to sync all my data locally, as it is proposed during the initial setup of the client. so i deleted this first â€œsync pairâ€?, and created new pairs with individual folders.\r\n\r\ni have to precise too, that for test purpose, i made this on a single computer, with the same client (which should sync the same folder on the server with 2 localization on the same computer)\r\n\r\na lot of subject have been opened on this issue, whereas i'm not sure they have the same origin:\r\nhttps://github.com/owncloud/mirall/issues/110\r\nhttps://github.com/owncloud/mirall/issues/49\r\nhttps://github.com/owncloud/mirall/issues/779\r\nhttp://forum.owncloud.org/viewtopic.php?f=14&t=15493&p=40791#p40791\r\n\r\nthanks for your help!\r\n\r\netienne"},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/25003251","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-25003251","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":25003251,"user":{"login":"bugsyb","id":5527773,"avatar_url":"https://gravatar.com/avatar/688e98427ce79152b9262dd3dbe8b692?d=https%3a%2f%2fidenticons.github.com%2f6bcd64df5e95b9290ee6d68136d3e6d8.png&r=x","gravatar_id":"688e98427ce79152b9262dd3dbe8b692","url":"https://api.github.com/users/bugsyb","html_url":"https://github.com/bugsyb","followers_url":"https://api.github.com/users/bugsyb/followers","following_url":"https://api.github.com/users/bugsyb/following{/other_user}","gists_url":"https://api.github.com/users/bugsyb/gists{/gist_id}","starred_url":"https://api.github.com/users/bugsyb/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bugsyb/subscriptions","organizations_url":"https://api.github.com/users/bugsyb/orgs","repos_url":"https://api.github.com/users/bugsyb/repos","events_url":"https://api.github.com/users/bugsyb/events{/privacy}","received_events_url":"https://api.github.com/users/bugsyb/received_events","type":"user","site_admin":false},"created_at":"2013-09-24t13:33:44z","updated_at":"2013-09-24t13:33:44z","body":"+1 for implementing hash based verification of the content as there's nothing like voting system"},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/25461414","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-25461414","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":25461414,"user":{"login":"aykutcevik","id":1965251,"avatar_url":"https://gravatar.com/avatar/573668205864e53140506e5608e8a729?d=https%3a%2f%2fidenticons.github.com%2fdf8f55aa858b34ada8f3eed54582245f.png&r=x","gravatar_id":"573668205864e53140506e5608e8a729","url":"https://api.github.com/users/aykutcevik","html_url":"https://github.com/aykutcevik","followers_url":"https://api.github.com/users/aykutcevik/followers","following_url":"https://api.github.com/users/aykutcevik/following{/other_user}","gists_url":"https://api.github.com/users/aykutcevik/gists{/gist_id}","starred_url":"https://api.github.com/users/aykutcevik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aykutcevik/subscriptions","organizations_url":"https://api.github.com/users/aykutcevik/orgs","repos_url":"https://api.github.com/users/aykutcevik/repos","events_url":"https://api.github.com/users/aykutcevik/events{/privacy}","received_events_url":"https://api.github.com/users/aykutcevik/received_events","type":"user","site_admin":false},"created_at":"2013-10-01t15:47:17z","updated_at":"2013-10-01t15:47:17z","body":"is there any workaround for now until the md5 based solution will be developed? having big troubles due to resyncs..."},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/25462243","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-25462243","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":25462243,"user":{"login":"etiess","id":900280,"avatar_url":"https://gravatar.com/avatar/a16a4ae7279e14f0c608a8e6e9599276?d=https%3a%2f%2fidenticons.github.com%2f2544b8d9a1b6b72b456adc440616c148.png&r=x","gravatar_id":"a16a4ae7279e14f0c608a8e6e9599276","url":"https://api.github.com/users/etiess","html_url":"https://github.com/etiess","followers_url":"https://api.github.com/users/etiess/followers","following_url":"https://api.github.com/users/etiess/following{/other_user}","gists_url":"https://api.github.com/users/etiess/gists{/gist_id}","starred_url":"https://api.github.com/users/etiess/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/etiess/subscriptions","organizations_url":"https://api.github.com/users/etiess/orgs","repos_url":"https://api.github.com/users/etiess/repos","events_url":"https://api.github.com/users/etiess/events{/privacy}","received_events_url":"https://api.github.com/users/etiess/received_events","type":"user","site_admin":false},"created_at":"2013-10-01t15:53:53z","updated_at":"2013-10-01t15:53:53z","body":"@aykutcevik for the moment unfortunately not.\r\n\r\nyou can follow https://github.com/owncloud/mirall/issues/994 and post your logs to help the team."},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/25788707","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-25788707","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":25788707,"user":{"login":"zatricky","id":3970906,"avatar_url":"https://gravatar.com/avatar/8f1abbebfccca89beabcc58bda7887c1?d=https%3a%2f%2fidenticons.github.com%2f704290d64eb633787cca98b863c490c5.png&r=x","gravatar_id":"8f1abbebfccca89beabcc58bda7887c1","url":"https://api.github.com/users/zatricky","html_url":"https://github.com/zatricky","followers_url":"https://api.github.com/users/zatricky/followers","following_url":"https://api.github.com/users/zatricky/following{/other_user}","gists_url":"https://api.github.com/users/zatricky/gists{/gist_id}","starred_url":"https://api.github.com/users/zatricky/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zatricky/subscriptions","organizations_url":"https://api.github.com/users/zatricky/orgs","repos_url":"https://api.github.com/users/zatricky/repos","events_url":"https://api.github.com/users/zatricky/events{/privacy}","received_events_url":"https://api.github.com/users/zatricky/received_events","type":"user","site_admin":false},"created_at":"2013-10-07t07:11:30z","updated_at":"2013-10-07t07:11:30z","body":"my apologies for the excess comment-traffic - i'd intended on suggesting using multiple hashsums but forgot to mention it.\r\n\r\ni'd suggest using md5 as a primary hashsum and using sha256 or sha512 as the second.\r\n\r\nthough not a security issue, per se, i would not use md5sum alone for the reasons cited here:\r\nhttps://en.wikipedia.org/wiki/md5#security"},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/25789086","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-25789086","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":25789086,"user":{"login":"karlitschek","id":867445,"avatar_url":"https://gravatar.com/avatar/87ce2b4531ee1a0c32b50e0d8f049224?d=https%3a%2f%2fidenticons.github.com%2f06e7098636caebfc9c30c9f52eb905df.png&r=x","gravatar_id":"87ce2b4531ee1a0c32b50e0d8f049224","url":"https://api.github.com/users/karlitschek","html_url":"https://github.com/karlitschek","followers_url":"https://api.github.com/users/karlitschek/followers","following_url":"https://api.github.com/users/karlitschek/following{/other_user}","gists_url":"https://api.github.com/users/karlitschek/gists{/gist_id}","starred_url":"https://api.github.com/users/karlitschek/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/karlitschek/subscriptions","organizations_url":"https://api.github.com/users/karlitschek/orgs","repos_url":"https://api.github.com/users/karlitschek/repos","events_url":"https://api.github.com/users/karlitschek/events{/privacy}","received_events_url":"https://api.github.com/users/karlitschek/received_events","type":"user","site_admin":false},"created_at":"2013-10-07t07:21:05z","updated_at":"2013-10-07t07:21:05z","body":"we maintain unique ids of the files on the server in the filesystem cache table and in the client sqlite database. a complete resync shoudn't happen unless the server or the client databases are changed or deleted somehow.\r\nany signs that this happened? "},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/25866875","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-25866875","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":25866875,"user":{"login":"zatricky","id":3970906,"avatar_url":"https://gravatar.com/avatar/8f1abbebfccca89beabcc58bda7887c1?d=https%3a%2f%2fidenticons.github.com%2f704290d64eb633787cca98b863c490c5.png&r=x","gravatar_id":"8f1abbebfccca89beabcc58bda7887c1","url":"https://api.github.com/users/zatricky","html_url":"https://github.com/zatricky","followers_url":"https://api.github.com/users/zatricky/followers","following_url":"https://api.github.com/users/zatricky/following{/other_user}","gists_url":"https://api.github.com/users/zatricky/gists{/gist_id}","starred_url":"https://api.github.com/users/zatricky/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zatricky/subscriptions","organizations_url":"https://api.github.com/users/zatricky/orgs","repos_url":"https://api.github.com/users/zatricky/repos","events_url":"https://api.github.com/users/zatricky/events{/privacy}","received_events_url":"https://api.github.com/users/zatricky/received_events","type":"user","site_admin":false},"created_at":"2013-10-08t06:09:16z","updated_at":"2013-10-08t06:09:16z","body":"@karlitschek: the use case inferred here is where the indexed value is based on a hash function. specifically, can the **unique id** identify a file based on the whole of its content or is it simply metadata that is independent of the actual file content?\r\n\r\nput another way: if i have two files with the same content, will their ids be identical?\r\n\r\nin this case the answer *must* be yes while also ensuring, within reasonable doubt, that we do not have two files with *different* content with the same id. (see http://git-scm.com/book/ch6-1.html#a-short-note-about-sha-1)\r\n\r\nrsync takes advantage of hashing to reduce the amount of data transferred in a sync. however, the use case is very different, and rsync does not keep a database of files on hand. in our case we want to leverage the client and server databases keeping these indexed hashes on hand to ensure that we do not re-upload or re-download content needlessly.\r\n\r\ni'm making a separate issue for consideration of full rsync-style synchronisation."},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/25867018","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-25867018","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":25867018,"user":{"login":"dragotin","id":1070214,"avatar_url":"https://gravatar.com/avatar/af829234b31fb915704c1c65ef48eee0?d=https%3a%2f%2fidenticons.github.com%2f3392afab7f424e10623ba95a941f4d34.png&r=x","gravatar_id":"af829234b31fb915704c1c65ef48eee0","url":"https://api.github.com/users/dragotin","html_url":"https://github.com/dragotin","followers_url":"https://api.github.com/users/dragotin/followers","following_url":"https://api.github.com/users/dragotin/following{/other_user}","gists_url":"https://api.github.com/users/dragotin/gists{/gist_id}","starred_url":"https://api.github.com/users/dragotin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dragotin/subscriptions","organizations_url":"https://api.github.com/users/dragotin/orgs","repos_url":"https://api.github.com/users/dragotin/repos","events_url":"https://api.github.com/users/dragotin/events{/privacy}","received_events_url":"https://api.github.com/users/dragotin/received_events","type":"user","site_admin":false},"created_at":"2013-10-08t06:14:10z","updated_at":"2013-10-08t06:14:10z","body":"the ids are not in relation with the file content. the are just metadata as you phrase it. two identical files will not have the same etag. we do not calculate a content based finger print because we have a multi backend structure which can make it very hard to read the whole file before syncing. "},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/25869742","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-25869742","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":25869742,"user":{"login":"zatricky","id":3970906,"avatar_url":"https://gravatar.com/avatar/8f1abbebfccca89beabcc58bda7887c1?d=https%3a%2f%2fidenticons.github.com%2f704290d64eb633787cca98b863c490c5.png&r=x","gravatar_id":"8f1abbebfccca89beabcc58bda7887c1","url":"https://api.github.com/users/zatricky","html_url":"https://github.com/zatricky","followers_url":"https://api.github.com/users/zatricky/followers","following_url":"https://api.github.com/users/zatricky/following{/other_user}","gists_url":"https://api.github.com/users/zatricky/gists{/gist_id}","starred_url":"https://api.github.com/users/zatricky/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zatricky/subscriptions","organizations_url":"https://api.github.com/users/zatricky/orgs","repos_url":"https://api.github.com/users/zatricky/repos","events_url":"https://api.github.com/users/zatricky/events{/privacy}","received_events_url":"https://api.github.com/users/zatricky/received_events","type":"user","site_admin":false},"created_at":"2013-10-08t07:29:00z","updated_at":"2013-10-08t07:29:00z","body":"ideally these hashes should be calculated by the client before the initial content upload. the server could have a scrub process to periodically verify these hashes (which would also help satisfy upgrade considerations). by storing these hashes in the database there will be little-to-no pre-sync i/o on the server. post-sync the server should probably verify the hashes within a short time-frame - but that is a decision i leave to the devs.\r\n\r\nin this use case of a \"pointless re-sync\", the only **work** the server will have to perform will be the sql queries necessary to see if the hashes being sent by the client already exist in the database."},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/25879578","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-25879578","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":25879578,"user":{"login":"dragotin","id":1070214,"avatar_url":"https://gravatar.com/avatar/af829234b31fb915704c1c65ef48eee0?d=https%3a%2f%2fidenticons.github.com%2f3392afab7f424e10623ba95a941f4d34.png&r=x","gravatar_id":"af829234b31fb915704c1c65ef48eee0","url":"https://api.github.com/users/dragotin","html_url":"https://github.com/dragotin","followers_url":"https://api.github.com/users/dragotin/followers","following_url":"https://api.github.com/users/dragotin/following{/other_user}","gists_url":"https://api.github.com/users/dragotin/gists{/gist_id}","starred_url":"https://api.github.com/users/dragotin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dragotin/subscriptions","organizations_url":"https://api.github.com/users/dragotin/orgs","repos_url":"https://api.github.com/users/dragotin/repos","events_url":"https://api.github.com/users/dragotin/events{/privacy}","received_events_url":"https://api.github.com/users/dragotin/received_events","type":"user","site_admin":false},"created_at":"2013-10-08t10:37:08z","updated_at":"2013-10-08t10:37:08z","body":"it's not that easy because:\r\n- not all content is uploaded through owncloud, imaging a samba share that is mounted into owncloud. people silently add, remove and change files on it.\r\n- because not all data changes go through the owncloud server, each content based fingerprint would have to be verified at access time.\r\n"},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/25892831","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-25892831","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":25892831,"user":{"login":"etiess","id":900280,"avatar_url":"https://gravatar.com/avatar/a16a4ae7279e14f0c608a8e6e9599276?d=https%3a%2f%2fidenticons.github.com%2f2544b8d9a1b6b72b456adc440616c148.png&r=x","gravatar_id":"a16a4ae7279e14f0c608a8e6e9599276","url":"https://api.github.com/users/etiess","html_url":"https://github.com/etiess","followers_url":"https://api.github.com/users/etiess/followers","following_url":"https://api.github.com/users/etiess/following{/other_user}","gists_url":"https://api.github.com/users/etiess/gists{/gist_id}","starred_url":"https://api.github.com/users/etiess/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/etiess/subscriptions","organizations_url":"https://api.github.com/users/etiess/orgs","repos_url":"https://api.github.com/users/etiess/repos","events_url":"https://api.github.com/users/etiess/events{/privacy}","received_events_url":"https://api.github.com/users/etiess/received_events","type":"user","site_admin":false},"created_at":"2013-10-08t14:09:12z","updated_at":"2013-10-08t14:09:12z","body":"hello @dragotin \r\n\r\nwhat are the backend structures which are not able to calculate hashes? do linux, mac, android, iphone and windows calculate hashes differently? is there no way to make these calculation compatible?\r\n\r\nif data changes don't go through oc server, then it should be possible to verify the hashes periodically or manually. or to verify them if other metadata changed (time, size, ...). or to force any new data to go through owncloud server. in any case, this is an unusual way to use oc for me, and it should not compromize the core function of synchronization.\r\n\r\nas you suggested me on http://dragotin.wordpress.com/2013/09/11/after-the-1-4-0-owncloud-client-release/ , i tried again (with 5.0.12 and 1.4.1) to copy an entire folder from a synced computer to a new computer to be synced (including csync_journal.db). then i configured the sync on the new computer. everything was downloaded again :-(. all logs and csync_journal.db (before and after the sync) are available here: https://www.sugarsync.com/pf/d6476655_61894308_919677\r\n\r\ni really think that this issue is much more important than the cases you mentioned. and i really think too that this issue would be solved using hash sums.\r\n"},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/25895916","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-25895916","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":25895916,"user":{"login":"moscicki","id":621965,"avatar_url":"https://gravatar.com/avatar/56282d408e288b61469589f1c98f4f86?d=https%3a%2f%2fidenticons.github.com%2fa73f312232a793606ee9b2c0615df705.png&r=x","gravatar_id":"56282d408e288b61469589f1c98f4f86","url":"https://api.github.com/users/moscicki","html_url":"https://github.com/moscicki","followers_url":"https://api.github.com/users/moscicki/followers","following_url":"https://api.github.com/users/moscicki/following{/other_user}","gists_url":"https://api.github.com/users/moscicki/gists{/gist_id}","starred_url":"https://api.github.com/users/moscicki/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/moscicki/subscriptions","organizations_url":"https://api.github.com/users/moscicki/orgs","repos_url":"https://api.github.com/users/moscicki/repos","events_url":"https://api.github.com/users/moscicki/events{/privacy}","received_events_url":"https://api.github.com/users/moscicki/received_events","type":"user","site_admin":false},"created_at":"2013-10-08t14:43:03z","updated_at":"2013-10-08t14:43:03z","body":"hello,\r\n\r\ni want to plug into this discussion. would you consider hashing of file metadata (mtime, size) instead of the content? such a hash could be used in the same way as the random etag, however it would have the advantage that you don't need to redownload files you already have. you could also trivially calculate hash on the secondary mounted storage on the server. on top, you could *calculate* the hash so if you lose local state db then you can simply recreate it (maybe you don't need it at for to store etags in this case).\r\nand then, you could possible also see if you can cut db use for etags on the server too. this could simply things big time and would make the system much more robust.\r\n\r\nhowever, it would require that mtimes be handled consistently on the clients - a client with a wrong clock could make a file appear on another client wit mtime/ctime in the future. but would not affect sync correctness in any way.\r\n\r\nbtw: how do you handle mimetype determination for secondary mounted storage on the server. do you maybe already read the first 256k bytes?\r\n\r\nwhat do you think?\r\n\r\nkuba\r\n\r\n--\r\n\r\non oct 8, 2013, at 4:09 pm, etiess <notifications@github.com> wrote:\r\n\r\n> hello @dragotin\r\n> \r\n> what are the backend structures which are not able to calculate hashes? do linux, mac, android, iphone and windows calculate hashes differently? is there no way to make these calculation compatible?\r\n> \r\n> if data changes don't go through oc server, then it should be possible to verify the hashes periodically or manually. or to verify them if other metadata changed (time, size, ...). or to force any new data to go through owncloud server. in any case, this is an unusual way to use oc for me, and it should not compromize the core function of synchronization.\r\n> \r\n> as you suggested me on http://dragotin.wordpress.com/2013/09/11/after-the-1-4-0-owncloud-client-release/ , i tried again (with 5.0.12 and 1.4.1) to copy an entire folder from a synced computer to a new computer to be synced (including csync_journal.db). then i configured the sync on the new computer. everything was downloaded again :-(. all logs and csync_journal.db (before and after the sync) are available here: https://www.sugarsync.com/pf/d6476655_61894308_919677\r\n> \r\n> i really think that this issue is much more important than the cases you mentioned. and i really think too that this issue would be solved using hash sums.\r\n> \r\n> â€”\r\n> reply to this email directly or view it on github.\r\n> "},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/25905875","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-25905875","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":25905875,"user":{"login":"zatricky","id":3970906,"avatar_url":"https://gravatar.com/avatar/8f1abbebfccca89beabcc58bda7887c1?d=https%3a%2f%2fidenticons.github.com%2f704290d64eb633787cca98b863c490c5.png&r=x","gravatar_id":"8f1abbebfccca89beabcc58bda7887c1","url":"https://api.github.com/users/zatricky","html_url":"https://github.com/zatricky","followers_url":"https://api.github.com/users/zatricky/followers","following_url":"https://api.github.com/users/zatricky/following{/other_user}","gists_url":"https://api.github.com/users/zatricky/gists{/gist_id}","starred_url":"https://api.github.com/users/zatricky/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zatricky/subscriptions","organizations_url":"https://api.github.com/users/zatricky/orgs","repos_url":"https://api.github.com/users/zatricky/repos","events_url":"https://api.github.com/users/zatricky/events{/privacy}","received_events_url":"https://api.github.com/users/zatricky/received_events","type":"user","site_admin":false},"created_at":"2013-10-08t16:32:52z","updated_at":"2013-10-08t16:37:26z","body":"thank you, @dragotin. i have to agree with etiess on the point that, while these hurdles are not easy (and i do appreciate that fact), the win gained from implementing hashing is incalculable. if my tone comes off as aggressive, i apologise. i'm having a hard time getting ideas across. if i was a php developer i'd have had poc patches in place within a weekend. unfortunately my \"good\" dev/engineering skills are limited to sql and bash. i'm an amateur when it comes to php. :-|\r\n\r\n@etiess, the hash support issue does not appear to be any platform-specific problem. it appears to simply be this:\r\ndragotin doth writ:\r\n> not all content is uploaded through owncloud, imaging a samba share that is mounted into owncloud. people silently add, remove and change files on it.\r\n\r\nthis certainly adds a small obstacle. zatricky commented:\r\n>  the server could have a scrub process to periodically verify these hashes (which would also help satisfy upgrade considerations)\r\n\r\nwith the above in mind, i don't see how it would be so hard to implement a \"have we got new files in here?\"-type check/scrub on the server. php supports inotify* which could help support this feature with very little i/o except on a first-run verification. checking for new files could even run once per minute while the verifications could run once a day/once a week)\r\n\r\n@moscicki lamented:\r\n> would you consider hashing of file metadata (mtime, size) instead of the content?\r\n\r\ni don't see how any performance issues/bandwidth waste would be mitigated by this. see below and please motivate further.\r\n\r\n> however it would have the advantage that you don't need to redownload files you already have\r\n\r\nthe ctime/filename don't take that much time/bandwidth/io to look up. the issue with re-downloading the content is still there in the simplest of cases: 1. have my desktop set up with oc. 2. copy a 8gb file from my desktop to my laptop. 3. touch and rename the file. 4. add the laptop to oc. 5. wait for africa's interwebs to catch up with a carrier pigeon.\r\n\r\n* http://php.net/manual/en/book.inotify.php"},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/26006984","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-26006984","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":26006984,"user":{"login":"randolfcarter","id":2631274,"avatar_url":"https://gravatar.com/avatar/e4a5d1c2c314d59e880d83b497a0d719?d=https%3a%2f%2fidenticons.github.com%2f3c3d7236785996f9e5f172ea5cdac91b.png&r=x","gravatar_id":"e4a5d1c2c314d59e880d83b497a0d719","url":"https://api.github.com/users/randolfcarter","html_url":"https://github.com/randolfcarter","followers_url":"https://api.github.com/users/randolfcarter/followers","following_url":"https://api.github.com/users/randolfcarter/following{/other_user}","gists_url":"https://api.github.com/users/randolfcarter/gists{/gist_id}","starred_url":"https://api.github.com/users/randolfcarter/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/randolfcarter/subscriptions","organizations_url":"https://api.github.com/users/randolfcarter/orgs","repos_url":"https://api.github.com/users/randolfcarter/repos","events_url":"https://api.github.com/users/randolfcarter/events{/privacy}","received_events_url":"https://api.github.com/users/randolfcarter/received_events","type":"user","site_admin":false},"created_at":"2013-10-09t20:44:09z","updated_at":"2013-10-09t20:44:09z","body":"@moscicki hashing of file modification date is definitely not a good option.\r\nit would be exactly the same as considering it for comparison directly. and that has been used in previous versions of the client (< 1.1) and it didn't work properly - machine times can drift, not all file systems support the same resolution for those times (but i bet @dragotin or @danimo can tell you much more about that). and then also the times would have to be kept in sync; and to consider the times would not help anything for the use case of somebody wanting to set up sync for an existing big data folder; the modification dates would most certainly be different.\r\n\r\nconsidering the size might help, but only very little, because the size alone tells you pretty little (it isn't even a proper indicator of whether a file has changed; the only thing you can tell is that if the size changed, then the file has changed; but not the other way round). so in many cases you'd still have to check the file anyway.\r\n\r\nto put it very clearly: the only safe way to tell whether a file has changed (or two files are different) is to consider the file content - e.g. by comparing hash sums."},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/26028502","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-26028502","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":26028502,"user":{"login":"tenacioustechie","id":432170,"avatar_url":"https://gravatar.com/avatar/ef592380f8c825ba4c2a8e21927c2495?d=https%3a%2f%2fidenticons.github.com%2f28033a1d4df30a98198bcd9f2078989e.png&r=x","gravatar_id":"ef592380f8c825ba4c2a8e21927c2495","url":"https://api.github.com/users/tenacioustechie","html_url":"https://github.com/tenacioustechie","followers_url":"https://api.github.com/users/tenacioustechie/followers","following_url":"https://api.github.com/users/tenacioustechie/following{/other_user}","gists_url":"https://api.github.com/users/tenacioustechie/gists{/gist_id}","starred_url":"https://api.github.com/users/tenacioustechie/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tenacioustechie/subscriptions","organizations_url":"https://api.github.com/users/tenacioustechie/orgs","repos_url":"https://api.github.com/users/tenacioustechie/repos","events_url":"https://api.github.com/users/tenacioustechie/events{/privacy}","received_events_url":"https://api.github.com/users/tenacioustechie/received_events","type":"user","site_admin":false},"created_at":"2013-10-10t04:32:27z","updated_at":"2013-10-10t04:38:06z","body":"+1 for hash based syncing. \r\n\r\nperhaps it could be even used where the files are uploaded through sync or web interface, and use fallback to etag and timestamp where that is not the case?\r\n\r\ngit actually uses hash's on directory contents too, so a hash of list of file hashes in a directory to understand when a directory changes. using this kind of hashing means you check one hash (at the top of the tree) and you can tell if any change in the tree has changed. this makes server to client checks very easy, obviously a more recursive approach is required on the client directory being synced by the sync client given it (i assume) doesn't get notified when a file gets changed. \r\n\r\nidentifying changes via hashing of the contents of the file is definitional a prior proven way to sync which numerous other systems that use (git, hg etc as well as other syncing systems i'm not aware of). \r\n\r\ni'm no php dev either, and i'm sure it's no small task especially given the prior architectural decisions like samba backends. \r\n\r\nthanks for all countless hours of development and the people providing feedback and log files etc. "},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/26031433","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-26031433","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":26031433,"user":{"login":"moscicki","id":621965,"avatar_url":"https://gravatar.com/avatar/56282d408e288b61469589f1c98f4f86?d=https%3a%2f%2fidenticons.github.com%2fa73f312232a793606ee9b2c0615df705.png&r=x","gravatar_id":"56282d408e288b61469589f1c98f4f86","url":"https://api.github.com/users/moscicki","html_url":"https://github.com/moscicki","followers_url":"https://api.github.com/users/moscicki/followers","following_url":"https://api.github.com/users/moscicki/following{/other_user}","gists_url":"https://api.github.com/users/moscicki/gists{/gist_id}","starred_url":"https://api.github.com/users/moscicki/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/moscicki/subscriptions","organizations_url":"https://api.github.com/users/moscicki/orgs","repos_url":"https://api.github.com/users/moscicki/repos","events_url":"https://api.github.com/users/moscicki/events{/privacy}","received_events_url":"https://api.github.com/users/moscicki/received_events","type":"user","site_admin":false},"created_at":"2013-10-10t06:19:00z","updated_at":"2013-10-10t06:19:00z","body":"@ randolfcarter:\r\n\r\ni think an efficient way of calculating etag while keeping it's required uniquness properties would provide such a great advantage to owncloud that it is worthwhile to investigate.\r\n> it would be exactly the same as considering it for comparison directly. and that has been used in previous versions of the client (< 1.1) and it didn't work properly - machine times can drift, not all file systems support the same resolution for those times (but i bet @dragotin or @danimo can tell you much more about that). and then also the times would have to be kept in sync; and to consider the times would not help anything for the use case of somebody wanting to set up sync for an existing big data folder; the modification dates would most certainly be different.\r\n> \r\nyou remark is perfectly valid. however what you want is to compare etags for equality (mtime1==mtime2) and not reason about time ordering (mtime1<mtime2). now, it is not impossible that two different clients touch the file and the resulting mtime will be exactly the same (well, it depends on the resolution, agreed) - but on modern systems where resolution in ms - how likely is that (also combined with the file size check)? etags must be unique to identify a file modification on the server, that's all.\r\n\r\n@dragotin, @danimo: there is a problem with low mtime resolution on some filesystems for the sync client anyway (e.g. on fat) - how do you detect local changes there?\r\n\r\nyou do not need to keep times in sync between the clients  - you only need to set mtime of the files consistently when file changes are propagated from the server. of course there is a side-effect: if your clocks are too skewed then files  fetched from the server may have mtime in the future.  if i upload an existing big data folder for sync - that's fine, i see no problem.\r\n> considering the size might help, but only very little, because the size alone tells you pretty little (it isn't even a proper indicator of whether a file has changed; the only thing you can tell is that if the size changed, then the file has changed; but not the other way round). so in many cases you'd still have to check the file anyway.\r\n> \r\n> to put it very clearly: the only safe way to tell whether a file has changed (or two files are different) is to consider the file content - e.g. by comparing hash sums.\r\n> \r\nthat's completely clear. \r\n\r\ndo you use hash sums to detect local changes on the sync client? i bet not. you take another approach which is \"good enough\". i would investigate if the same could not be done to etag in general.\r\n\r\non march 28, 2013 on owncloud@kde.org mailing list  i asked this question already: \r\n\r\n\"\"\"\r\ni am looking for a complete and up-to-date reference which describes the *intended* synchronization model in owncloud. do you know if there is one (apart from the ocsync source code)? in particular, what happens if time in not up-to-date on all clients, or if the clock on a client is manually adjusted (in the future or into the past) or if a clock drifts in time. this is normal in heterogeneous distributed environments.\r\n\"\"\"\r\n\r\nin other words: a short description of a conceptual model of sync in owncloud would also allow to get useful feedback from others - there are tons of smart people out there who would certainly suggest some smart ideas.\r\n\r\nkuba\r\n\r\n--"},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/26037577","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-26037577","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":26037577,"user":{"login":"karlitschek","id":867445,"avatar_url":"https://gravatar.com/avatar/87ce2b4531ee1a0c32b50e0d8f049224?d=https%3a%2f%2fidenticons.github.com%2f06e7098636caebfc9c30c9f52eb905df.png&r=x","gravatar_id":"87ce2b4531ee1a0c32b50e0d8f049224","url":"https://api.github.com/users/karlitschek","html_url":"https://github.com/karlitschek","followers_url":"https://api.github.com/users/karlitschek/followers","following_url":"https://api.github.com/users/karlitschek/following{/other_user}","gists_url":"https://api.github.com/users/karlitschek/gists{/gist_id}","starred_url":"https://api.github.com/users/karlitschek/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/karlitschek/subscriptions","organizations_url":"https://api.github.com/users/karlitschek/orgs","repos_url":"https://api.github.com/users/karlitschek/repos","events_url":"https://api.github.com/users/karlitschek/events{/privacy}","received_events_url":"https://api.github.com/users/karlitschek/received_events","type":"user","site_admin":false},"created_at":"2013-10-10t08:47:38z","updated_at":"2013-10-10t08:47:38z","body":"let me explain why we don't use a hash at the moment.\r\nthe etag is a unique id of a file. from a client or sync algorithm perspective this exactly the same as a hash. if the etag is the same than it is the same file, if it's different than it is a different file. just like a hash. so if this assumption is true than the syncing should work in exactly the same way as with a hash.\r\n\r\nin the current implementation that etag is calculated using the metadata of a file like mtime, name, ... the reason is performance. owncloud can be used with petabytes of storage. some of them could be access and changed independently from owncloud. just look at the external filesystem features as an example. you can mount your huge s3,ftp,cifs,dropbox,... storage into owncloud. if we want to calc hashes for every file than we have to download every single file at every sinc run to check if the hash/content of the file is the same. this is not possible obviously. because of that we only look at the metadata for the etag.\r\ni really think that this should be enough. i'm still waiting for a real life example where a file is changed in a directory but has still the same name,size,mtime, .... this really shouldn't happen.\r\nif there are sync problems than there must be a different reason for that that we have to debug and fix. \r\n"},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/26038460","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-26038460","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":26038460,"user":{"login":"zatricky","id":3970906,"avatar_url":"https://gravatar.com/avatar/8f1abbebfccca89beabcc58bda7887c1?d=https%3a%2f%2fidenticons.github.com%2f704290d64eb633787cca98b863c490c5.png&r=x","gravatar_id":"8f1abbebfccca89beabcc58bda7887c1","url":"https://api.github.com/users/zatricky","html_url":"https://github.com/zatricky","followers_url":"https://api.github.com/users/zatricky/followers","following_url":"https://api.github.com/users/zatricky/following{/other_user}","gists_url":"https://api.github.com/users/zatricky/gists{/gist_id}","starred_url":"https://api.github.com/users/zatricky/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zatricky/subscriptions","organizations_url":"https://api.github.com/users/zatricky/orgs","repos_url":"https://api.github.com/users/zatricky/repos","events_url":"https://api.github.com/users/zatricky/events{/privacy}","received_events_url":"https://api.github.com/users/zatricky/received_events","type":"user","site_admin":false},"created_at":"2013-10-10t09:04:12z","updated_at":"2013-10-10t09:04:12z","body":"@moscicki said:\r\n> do you use hash sums to detect local changes on the sync client? i bet not. you take another approach which is \"good enough\". i would investigate if the same could not be done to etag in general.\r\n\r\netags are important but *cannot* resolve the problem of needlessly re-uploading/downloading content which has already been manually/externally synchronised.\r\n\r\nre ensuring mtime comparisons stay in sync, there is no easy answer. instead, i think oc bypasses this  problem by only considering mtime at the time of the initial upload/download. this is, i imagine, why the etags were put in place. we don't need to worry about comparing mtime between client and server as long as:\r\n* server's database mtime matches the server's local filesystem mtime\r\n* client's database mtime matches the client's local filesystem mtime\r\n* client's database etag matches the server's database etag\r\n\r\nhowever, if the client database has a different mtime to its filesystem, it knows the content **might** have changed. the client then regenerates an etag and triggers an upload (which **might** be undesired).\r\n@dragotin / @danimo @karlitschek - i'd appreciate any comment on the accuracy of above. ;)\r\n\r\nthe above brings about the scenario where a file is touched but the content is not changed. the mtime being updated triggers an unnecessary upload. the other problem we have (and the reason this bug/issue exists) is where we synchronise content externally, the etag does not exist on the client database, and there is currently no way to tell the client that the content it has is identical to the content already on the server. a hash is the only way to rectify this behaviour.\r\n\r\n*with* a hash, we see the mtime has changed, triggering a recalculation of the hash. we see that the content has *not* changed and we **do not** pointlessly re-upload the arbitrarily large file. the only steps we might still take, depending on dev decisions, would be:\r\n* tell the server to update the timestamp\r\n* tell the server the etag has been updated (might be necessary to trigger mtime updates on all clients)\r\n\r\n>in other words: a short description of a conceptual model of sync in owncloud would also allow to get useful feedback from others - there are tons of smart people out there who would certainly suggest some smart ideas.\r\n\r\n+1. it would be helpful to have a reference/concept document, even if it is not necessarily easy to read."},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/26038659","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-26038659","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":26038659,"user":{"login":"zatricky","id":3970906,"avatar_url":"https://gravatar.com/avatar/8f1abbebfccca89beabcc58bda7887c1?d=https%3a%2f%2fidenticons.github.com%2f704290d64eb633787cca98b863c490c5.png&r=x","gravatar_id":"8f1abbebfccca89beabcc58bda7887c1","url":"https://api.github.com/users/zatricky","html_url":"https://github.com/zatricky","followers_url":"https://api.github.com/users/zatricky/followers","following_url":"https://api.github.com/users/zatricky/following{/other_user}","gists_url":"https://api.github.com/users/zatricky/gists{/gist_id}","starred_url":"https://api.github.com/users/zatricky/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zatricky/subscriptions","organizations_url":"https://api.github.com/users/zatricky/orgs","repos_url":"https://api.github.com/users/zatricky/repos","events_url":"https://api.github.com/users/zatricky/events{/privacy}","received_events_url":"https://api.github.com/users/zatricky/received_events","type":"user","site_admin":false},"created_at":"2013-10-10t09:08:07z","updated_at":"2013-10-10t09:08:50z","body":"@karlitschek - ah, thanks. that makes a huge difference in those use cases.\r\n\r\ni guess the next question is regarding @danimo's comment earlier in the thread:\r\n> note: we could at least have hash sums for the files that we have exclusive access to.\r\n\r\ni don't see a simple way to automatically differentiate between \"local\" storage (san/local disk/raid) and \"remote\" storage (s3/ftp/etc)."},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/26039738","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-26039738","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":26039738,"user":{"login":"etiess","id":900280,"avatar_url":"https://gravatar.com/avatar/a16a4ae7279e14f0c608a8e6e9599276?d=https%3a%2f%2fidenticons.github.com%2f2544b8d9a1b6b72b456adc440616c148.png&r=x","gravatar_id":"a16a4ae7279e14f0c608a8e6e9599276","url":"https://api.github.com/users/etiess","html_url":"https://github.com/etiess","followers_url":"https://api.github.com/users/etiess/followers","following_url":"https://api.github.com/users/etiess/following{/other_user}","gists_url":"https://api.github.com/users/etiess/gists{/gist_id}","starred_url":"https://api.github.com/users/etiess/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/etiess/subscriptions","organizations_url":"https://api.github.com/users/etiess/orgs","repos_url":"https://api.github.com/users/etiess/repos","events_url":"https://api.github.com/users/etiess/events{/privacy}","received_events_url":"https://api.github.com/users/etiess/received_events","type":"user","site_admin":false},"created_at":"2013-10-10t09:27:28z","updated_at":"2013-10-10t09:31:09z","body":"thank you @karlitschek for your explanation: we understand the advantages of etag and local db. but https://github.com/owncloud/mirall/issues/994 shows that it is not robust enough, and that a problem with the db would cause massive download.\r\n\r\ni think @zatricky points out a good idea in the case where the etag has changed (for a good reason, or because of a corruption of the db):\r\n<blockquote>\r\n<p>with a hash, we see the mtime has changed, triggering a recalculation of the hash. we see that the content has not changed and we do not pointlessly re-upload the arbitrarily large file</p>\r\n</blockquote>\r\n\r\nis it worth considering both local db/etag and (if the etag has changed) hash?"},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/26115679","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-26115679","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":26115679,"user":{"login":"moscicki","id":621965,"avatar_url":"https://gravatar.com/avatar/56282d408e288b61469589f1c98f4f86?d=https%3a%2f%2fidenticons.github.com%2fa73f312232a793606ee9b2c0615df705.png&r=x","gravatar_id":"56282d408e288b61469589f1c98f4f86","url":"https://api.github.com/users/moscicki","html_url":"https://github.com/moscicki","followers_url":"https://api.github.com/users/moscicki/followers","following_url":"https://api.github.com/users/moscicki/following{/other_user}","gists_url":"https://api.github.com/users/moscicki/gists{/gist_id}","starred_url":"https://api.github.com/users/moscicki/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/moscicki/subscriptions","organizations_url":"https://api.github.com/users/moscicki/orgs","repos_url":"https://api.github.com/users/moscicki/repos","events_url":"https://api.github.com/users/moscicki/events{/privacy}","received_events_url":"https://api.github.com/users/moscicki/received_events","type":"user","site_admin":false},"created_at":"2013-10-11t05:37:42z","updated_at":"2013-10-11t05:37:42z","body":"> do you use hash sums to detect local changes on the sync client? i bet not. you take another approach which is \"good enough\". i would investigate if the same could not be done to etag in general.\r\n> \r\n> etags are important but cannot resolve the problem of needlessly re-uploading/downloading content which has already been manually/externally synchronised.\r\n> \r\n@zatricky:\r\n\r\netags which may be calculated can solve your problem because you don't care if you loose local sync db. i understand frank's reasons for not hashing the content in a general case. it may also put extra load on the clients (which already now occasionally tend to consume 100% cpu). my point is that if we can calculate the etags which are unique enough based on the metadata both you and frank may be happy (to some extent at least ;-)). and i would be happy too.\r\n\r\n@karlitschek:\r\n\r\noptimization of specific cases is a different story - we consider using a storage backend which does content checksums automatically for us. owncloud attempts a beautiful thing with a very generic framework - it would be ideal the framework would also allow to handle optimally particular setups taking advantage of capabilities available at lower levels. also for storage which allows extended attributes. this also applies to using owncloud with local disk backend assuming that noone else writes into it. otherwise we will have to live with a least common denominator from all possible use-cases. i know this is not easy but asymptotically imo this framework probably needs to go in this direction somehow. \r\n> re ensuring mtime comparisons stay in sync, there is no easy answer. instead, i think oc bypasses this problem by only considering mtime at the time of the initial upload/download. this is, i imagine, why the etags were put in place. we don't need to worry about comparing mtime between client and server as long as:\r\n> \r\n> server's database mtime matches the server's local filesystem mtime\r\n> client's database mtime matches the client's local filesystem mtime\r\n> client's database etag matches the server's database etag\r\n> however, if the client database has a different mtime to its filesystem, it knows the content might have changed. the client then regenerates an etag and triggers an upload (which might be undesired).\r\n> @dragotin / @danimo @karlitschek - i'd appreciate any comment on the accuracy of above. ;)\r\n> \r\nthis is also how i understood it - but i am a mere user ;-)\r\n\r\nhowever i just did an experiment which shows that mtimes are propagated between the clients (linux) albeit in a way which cannot be used for reliable hashing:\r\n\r\n1. created file as client a:\r\n\r\n  file: `etag/x'\r\n  size: 0         \tblocks: 0          io block: 4096   regular empty file\r\ndevice: 901h/2305d\tinode: 25035386    links: 1\r\naccess: (0644/-rw-r--r--)  uid: (    0/    root)   gid: (    0/    root)\r\naccess: 2013-10-11 06:53:51.313837559 +0200\r\nmodify: 2013-10-11 06:53:51.313837559 +0200\r\nchange: 2013-10-11 06:53:51.313837559 +0200\r\n\r\n2. synced to the server (same physical host)\r\n\r\n3. stat the file stored on the server:\r\n\r\n  file: `/boxstorage/etag/files/x'\r\n  size: 0         \tblocks: 0          io block: 4096   regular empty file\r\ndevice: 901h/2305d\tinode: 27264315    links: 1\r\naccess: (0644/-rw-r--r--)  uid: (   48/  apache)   gid: (   48/  apache)\r\naccess: 2013-10-11 06:53:51.000000000 +0200\r\nmodify: 2013-10-11 06:53:51.000000000 +0200\r\nchange: 2013-10-11 06:54:15.324439746 +0200\r\n\r\n4. synced as client b (same physical host) - mtime is set as on the server:\r\n\r\n  file: `etag-2/x'\r\n  size: 0         \tblocks: 0          io block: 4096   regular empty file\r\ndevice: 901h/2305d\tinode: 25035391    links: 1\r\naccess: (0644/-rw-r--r--)  uid: (    0/    root)   gid: (    0/    root)\r\naccess: 2013-10-11 06:53:51.000000000 +0200\r\nmodify: 2013-10-11 06:53:51.000000000 +0200\r\nchange: 2013-10-11 06:55:11.403174968 +0200\r\n\r\n5. and further on, upon changes and syncs the mtime propagates between the clients with 1s mtime resolution \r\n\r\nso as one sees in the example above mtime is propagated but not consistent - client a mtime is not the same as client b mtime. that's why it cannot be relied upon and hashed effectively. not sure if there is an easy way out in all cases because it needs to support various filesystem limitations on the client side. however, imo, if a filesystem has a capability then it should be exploited.\r\n\r\n> the above brings about the scenario where a file is touched but the content is not changed. the mtime being updated triggers an unnecessary upload. the other problem we have (and the reason this bug/issue exists) is where we synchronise content externally, the etag does not exist on the client database, and there is currently no way to tell the client that the content it has is identical to the content already on the server. a hash is the only way to rectify this behaviour.\r\n> \r\nok, touching a file and getting a redownload - it is not optimal but somewhat acceptable for me - at least in the current version of owncloud. losing local state db or getting it corrupted that and getting a full redownload is really suboptimal.\r\n\r\nkuba\r\n\r\n--\r\n\r\n> with a hash, we see the mtime has changed, triggering a recalculation of the hash. we see that the content has not changed and we do not pointlessly re-upload the arbitrarily large file. the only steps we might still take, depending on dev decisions, would be:\r\n> \r\n> tell the server to update the timestamp\r\n> tell the server the etag has been updated (might be necessary to trigger mtime updates on all clients)\r\n> in other words: a short description of a conceptual model of sync in owncloud would also allow to get useful feedback from others - there are tons of smart people out there who would certainly suggest some smart ideas.\r\n> \r\n> +1. it would be helpful to have a reference/concept document, even if it is not necessarily easy to read.\r\n> \r\n> â€”\r\n> reply to this email directly or view it on github.\r\n> "},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/26120433","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-26120433","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":26120433,"user":{"login":"dragotin","id":1070214,"avatar_url":"https://gravatar.com/avatar/af829234b31fb915704c1c65ef48eee0?d=https%3a%2f%2fidenticons.github.com%2f3392afab7f424e10623ba95a941f4d34.png&r=x","gravatar_id":"af829234b31fb915704c1c65ef48eee0","url":"https://api.github.com/users/dragotin","html_url":"https://github.com/dragotin","followers_url":"https://api.github.com/users/dragotin/followers","following_url":"https://api.github.com/users/dragotin/following{/other_user}","gists_url":"https://api.github.com/users/dragotin/gists{/gist_id}","starred_url":"https://api.github.com/users/dragotin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dragotin/subscriptions","organizations_url":"https://api.github.com/users/dragotin/orgs","repos_url":"https://api.github.com/users/dragotin/repos","events_url":"https://api.github.com/users/dragotin/events{/privacy}","received_events_url":"https://api.github.com/users/dragotin/received_events","type":"user","site_admin":false},"created_at":"2013-10-11t07:58:55z","updated_at":"2013-10-11t07:58:55z","body":"i haven't yet thought through the whole conversation but here are some facts:\r\n\r\n- there is **no** full re-download of data required if the client database is lost. if the client db is missing, the file name and mtimes are compared and if they are equal, the files are not downloaded again. \r\n- mtimes are propagated as epoch values with a second precision if the system provides that, there are rumors that windows file systems only provide a two seconds precision.\r\n- note that even if the system clocks of involved systems are not equal, the mtime of an individual file is not affected by that because that is just a number basically.\r\n\r\nthe idea to calculate the checksum of the file content of a file on client side to avoid re-upload if the file was not really changed but just touched is something to consider. however, i wonder if this is not more an academic than a practical problem, most users probably don't use touch that regularly. on files."},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/26124543","html_url":"https://github.com/owncloud/core/issues/523#issuecomment-26124543","issue_url":"https://api.github.com/repos/owncloud/core/issues/523","id":26124543,"user":{"login":"zatricky","id":3970906,"avatar_url":"https://gravatar.com/avatar/8f1abbebfccca89beabcc58bda7887c1?d=https%3a%2f%2fidenticons.github.com%2f704290d64eb633787cca98b863c490c5.png&r=x","gravatar_id":"8f1abbebfccca89beabcc58bda7887c1","url":"https://api.github.com/users/zatricky","html_url":"https://github.com/zatricky","followers_url":"https://api.github.com/users/zatricky/followers","following_url":"https://api.github.com/users/zatricky/following{/other_user}","gists_url":"https://api.github.com/users/zatricky/gists{/gist_id}","starred_url":"https://api.github.com/users/zatricky/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zatricky/subscriptions","organizations_url":"https://api.github.com/users/zatricky/orgs","repos_url":"https://api.github.com/users/zatricky/repos","events_url":"https://api.github.com/users/zatricky/events{/privacy}","received_events_url":"https://api.github.com/users/zatricky/received_events","type":"user","site_admin":false},"created_at":"2013-10-11t09:21:44z","updated_at":"2013-10-11t09:21:44z","body":"thanks for the info, @dragotin. that helps understand the current behaviour better.\r\n\r\nmy using 'touch' was merely to demonstrate how simple reproducing the problem can be. i've reproduced changing the mtime in cmdline without using touch. this is simply done by copying/overwriting files. this covers #5231 as well as this issue's original submitter: http://sprunge.us/bbgb"}]