[{"url":"https://api.github.com/repos/owncloud/core/issues/comments/23797947","html_url":"https://github.com/owncloud/core/issues/4718#issuecomment-23797947","issue_url":"https://api.github.com/repos/owncloud/core/issues/4718","id":23797947,"user":{"login":"blizzz","id":2184312,"avatar_url":"https://gravatar.com/avatar/1898b25a9cb3aa1a0f0febd1359910b9?d=https%3a%2f%2fidenticons.github.com%2fca03acec6deadf508b75eb0cf9ba036c.png&r=x","gravatar_id":"1898b25a9cb3aa1a0f0febd1359910b9","url":"https://api.github.com/users/blizzz","html_url":"https://github.com/blizzz","followers_url":"https://api.github.com/users/blizzz/followers","following_url":"https://api.github.com/users/blizzz/following{/other_user}","gists_url":"https://api.github.com/users/blizzz/gists{/gist_id}","starred_url":"https://api.github.com/users/blizzz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/blizzz/subscriptions","organizations_url":"https://api.github.com/users/blizzz/orgs","repos_url":"https://api.github.com/users/blizzz/repos","events_url":"https://api.github.com/users/blizzz/events{/privacy}","received_events_url":"https://api.github.com/users/blizzz/received_events","type":"user","site_admin":false},"created_at":"2013-09-04t15:20:37z","updated_at":"2013-09-04t15:20:37z","body":"@butonic "},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/23807679","html_url":"https://github.com/owncloud/core/issues/4718#issuecomment-23807679","issue_url":"https://api.github.com/repos/owncloud/core/issues/4718","id":23807679,"user":{"login":"butonic","id":956847,"avatar_url":"https://gravatar.com/avatar/9760c134cabf66e10dbba06d7525f5c7?d=https%3a%2f%2fidenticons.github.com%2f92925ec1ad893739ed57986f18e2f060.png&r=x","gravatar_id":"9760c134cabf66e10dbba06d7525f5c7","url":"https://api.github.com/users/butonic","html_url":"https://github.com/butonic","followers_url":"https://api.github.com/users/butonic/followers","following_url":"https://api.github.com/users/butonic/following{/other_user}","gists_url":"https://api.github.com/users/butonic/gists{/gist_id}","starred_url":"https://api.github.com/users/butonic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/butonic/subscriptions","organizations_url":"https://api.github.com/users/butonic/orgs","repos_url":"https://api.github.com/users/butonic/repos","events_url":"https://api.github.com/users/butonic/events{/privacy}","received_events_url":"https://api.github.com/users/butonic/received_events","type":"user","site_admin":false},"created_at":"2013-09-04t17:24:18z","updated_at":"2013-09-04t17:24:18z","body":"currently search lucene needs to load the full pdf into ram. it should mark the pdf as failed in the db and continue with the next pdf. click in the search field to see the progress. a status log in the users settings page is on the roadmap. until then you will have to find the pdf by looking in the search lucene status table.\r\n\r\ni don't think this is a problem of too many pdfs but of one pdf too large for the available ram.\r\n\r\n@choeger can you verify that?"},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/24459167","html_url":"https://github.com/owncloud/core/issues/4718#issuecomment-24459167","issue_url":"https://api.github.com/repos/owncloud/core/issues/4718","id":24459167,"user":{"login":"ownhardy","id":4622654,"avatar_url":"https://identicons.github.com/0828a73154f5b71f38f20b33e631eb13.png","gravatar_id":null,"url":"https://api.github.com/users/ownhardy","html_url":"https://github.com/ownhardy","followers_url":"https://api.github.com/users/ownhardy/followers","following_url":"https://api.github.com/users/ownhardy/following{/other_user}","gists_url":"https://api.github.com/users/ownhardy/gists{/gist_id}","starred_url":"https://api.github.com/users/ownhardy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ownhardy/subscriptions","organizations_url":"https://api.github.com/users/ownhardy/orgs","repos_url":"https://api.github.com/users/ownhardy/repos","events_url":"https://api.github.com/users/ownhardy/events{/privacy}","received_events_url":"https://api.github.com/users/ownhardy/received_events","type":"user","site_admin":false},"created_at":"2013-09-14t21:07:38z","updated_at":"2013-09-14t21:07:38z","body":"i had the same error message with just one single and large pdf file (55 mb in my case). \r\n\r\nthis error is not so nice because i consider my owncloud useful for the purpose to share large files with others which are too big to be sent as an email attachment (as the above mentioned pdf file...)\r\n\r\ni use owncloud 5.0.10 an a shared linux server (sqlite and php 5.3.24)"},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/30216613","html_url":"https://github.com/owncloud/core/issues/4718#issuecomment-30216613","issue_url":"https://api.github.com/repos/owncloud/core/issues/4718","id":30216613,"user":{"login":"butonic","id":956847,"avatar_url":"https://gravatar.com/avatar/9760c134cabf66e10dbba06d7525f5c7?d=https%3a%2f%2fidenticons.github.com%2f92925ec1ad893739ed57986f18e2f060.png&r=x","gravatar_id":"9760c134cabf66e10dbba06d7525f5c7","url":"https://api.github.com/users/butonic","html_url":"https://github.com/butonic","followers_url":"https://api.github.com/users/butonic/followers","following_url":"https://api.github.com/users/butonic/following{/other_user}","gists_url":"https://api.github.com/users/butonic/gists{/gist_id}","starred_url":"https://api.github.com/users/butonic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/butonic/subscriptions","organizations_url":"https://api.github.com/users/butonic/orgs","repos_url":"https://api.github.com/users/butonic/repos","events_url":"https://api.github.com/users/butonic/events{/privacy}","received_events_url":"https://api.github.com/users/butonic/received_events","type":"user","site_admin":false},"created_at":"2013-12-10t11:04:02z","updated_at":"2013-12-10t11:04:02z","body":"well i think that the current behavior is correct. it will try to index a large pdf. when it fails it logs the error and marks the pdf as such, so it won't be indexed again. @ownhardy how does this problem prevent you from sharing large pdfs to other people? what is your workflow. for oc7 i plan to improve the search functionality a lot, so your feedback is welcome."},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/30451663","html_url":"https://github.com/owncloud/core/issues/4718#issuecomment-30451663","issue_url":"https://api.github.com/repos/owncloud/core/issues/4718","id":30451663,"user":{"login":"ownhardy","id":4622654,"avatar_url":"https://identicons.github.com/0828a73154f5b71f38f20b33e631eb13.png","gravatar_id":null,"url":"https://api.github.com/users/ownhardy","html_url":"https://github.com/ownhardy","followers_url":"https://api.github.com/users/ownhardy/followers","following_url":"https://api.github.com/users/ownhardy/following{/other_user}","gists_url":"https://api.github.com/users/ownhardy/gists{/gist_id}","starred_url":"https://api.github.com/users/ownhardy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ownhardy/subscriptions","organizations_url":"https://api.github.com/users/ownhardy/orgs","repos_url":"https://api.github.com/users/ownhardy/repos","events_url":"https://api.github.com/users/ownhardy/events{/privacy}","received_events_url":"https://api.github.com/users/ownhardy/received_events","type":"user","site_admin":false},"created_at":"2013-12-12t19:09:16z","updated_at":"2013-12-12t19:09:16z","body":"@butonic : \r\n* the mentioned error message did not prevent me from sharing this large file with someone else. that worked fine.\r\n* my workflow is as follows: i upload a large file to oc. then i create a \"share link\" in oc and send it via email to someone else. this person can download the file then. afterwards i delete the file again.\r\n* my only concern was the error message itself. \"fatal php error\" does not sound like something i like to have on my server. and now you tell me that this is the correct behavior. i personally would prefer the following behavior of oc: (1) check for each file if it is small enough to be indexed. (2) too big files could be marked for \"no indexing\", and maybe a warning (at least one step less than the current error level) could be issued. (3) all other files are properly indexed. ==> in general my personal understanding would be that a planned and correct behavior should never generate an error level this high."},{"url":"https://api.github.com/repos/owncloud/core/issues/comments/30464977","html_url":"https://github.com/owncloud/core/issues/4718#issuecomment-30464977","issue_url":"https://api.github.com/repos/owncloud/core/issues/4718","id":30464977,"user":{"login":"butonic","id":956847,"avatar_url":"https://gravatar.com/avatar/9760c134cabf66e10dbba06d7525f5c7?d=https%3a%2f%2fidenticons.github.com%2f92925ec1ad893739ed57986f18e2f060.png&r=x","gravatar_id":"9760c134cabf66e10dbba06d7525f5c7","url":"https://api.github.com/users/butonic","html_url":"https://github.com/butonic","followers_url":"https://api.github.com/users/butonic/followers","following_url":"https://api.github.com/users/butonic/following{/other_user}","gists_url":"https://api.github.com/users/butonic/gists{/gist_id}","starred_url":"https://api.github.com/users/butonic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/butonic/subscriptions","organizations_url":"https://api.github.com/users/butonic/orgs","repos_url":"https://api.github.com/users/butonic/repos","events_url":"https://api.github.com/users/butonic/events{/privacy}","received_events_url":"https://api.github.com/users/butonic/received_events","type":"user","site_admin":false},"created_at":"2013-12-12t21:43:45z","updated_at":"2013-12-12t21:43:45z","body":"@ownhardy yes i agree that a fatal error sounds pretty bad. and from a technical perspective it is ... the indexing completely fills up the ram and the request dies. unfortunately, i have not yet found a way to catch that \"php fatal error: allowed memory size\" or prevent logging it. not even registering a custom shutdown handler seems to be an option to prevent logging this. i'll try preventing extensive ram usage by unsetting some variables..."}]