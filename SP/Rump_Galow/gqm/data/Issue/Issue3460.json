{"url":"https://api.github.com/repos/owncloud/core/issues/3460","labels_url":"https://api.github.com/repos/owncloud/core/issues/3460/labels{/name}","comments_url":"https://api.github.com/repos/owncloud/core/issues/3460/comments","events_url":"https://api.github.com/repos/owncloud/core/issues/3460/events","html_url":"https://github.com/owncloud/core/issues/3460","id":14652172,"number":3460,"title":"web interface extremely slow in oc 5.x","user":{"login":"jonathanvaughn","id":1672327,"avatar_url":"https://gravatar.com/avatar/dafc3bc13805c15664fdf27f12217448?d=https%3a%2f%2fidenticons.github.com%2f43d00d8850b664c5e8786bbd7657aabd.png&r=x","gravatar_id":"dafc3bc13805c15664fdf27f12217448","url":"https://api.github.com/users/jonathanvaughn","html_url":"https://github.com/jonathanvaughn","followers_url":"https://api.github.com/users/jonathanvaughn/followers","following_url":"https://api.github.com/users/jonathanvaughn/following{/other_user}","gists_url":"https://api.github.com/users/jonathanvaughn/gists{/gist_id}","starred_url":"https://api.github.com/users/jonathanvaughn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jonathanvaughn/subscriptions","organizations_url":"https://api.github.com/users/jonathanvaughn/orgs","repos_url":"https://api.github.com/users/jonathanvaughn/repos","events_url":"https://api.github.com/users/jonathanvaughn/events{/privacy}","received_events_url":"https://api.github.com/users/jonathanvaughn/received_events","type":"user","site_admin":false},"labels":[],"state":"closed","assignee":null,"milestone":null,"comments":2,"created_at":"2013-05-23t00:14:21z","updated_at":"2013-05-23t01:08:51z","closed_at":"2013-05-23t01:08:51z","pull_request":{"html_url":null,"diff_url":null,"patch_url":null},"body":"### expected behaviour\r\nweb interface should not be terribly slow - no more than a few seconds page load time when listing the top level of the '/shared' directory is not unreasonable\r\n\r\n### actual behaviour\r\nweb interface was fine in 4.5.x. slowness is worst when at the top level of a share and gets better as you go down in the tree. apparently, owncloud is generating a list of all files and directories under the current path each time (so it takes ages at higher levels of the tree).\r\n\r\nthis may affect webdav too but typically our sync clients when they do complete a sync take 12-20 hours to do so even before the upgrade, so its hard to tell if it made a difference.\r\n\r\nwe've got over 60k dirs with over 600k files in them. most are under one account, which is where all the company-wide shares are shared from.\r\n\r\n    data                     66855   directories containing  634581  files, 214g\r\n    data/xxxxxxxxx           3       directories containing  0       files, 12k\r\n    data/xxxxxxxxxxx         64      directories containing  0       files, 256k\r\n    data/xxxxxxx             3488    directories containing  2       files, 14m\r\n    data/xxxxxxx             56366   directories containing  634532  files, 213g\r\n    data/xxxxxx              7       directories containing  0       files, 28k\r\n    data/xxxxxxx             4       directories containing  0       files, 16k\r\n    data/xxxxxxx             5       directories containing  0       files, 20k\r\n    data/xxxxxxx             11      directories containing  1       files, 68k\r\n\tdata/xxxxxxx             44      directories containing  0       files, 176k\r\n\tdata/xxxxxxxxx           3       directories containing  0       files, 12k\r\n\tdata/xxxxxxxxxxxxxx      9       directories containing  1       files, 60k\r\n\tdata/xxxxxxx             3       directories containing  0       files, 12k\r\n\tdata/xxxxxx              5       directories containing  0       files, 20k\r\n\tdata/xxxxxxxxxx          3       directories containing  0       files, 12k\r\n\tdata/xxxxxxx             32      directories containing  3       files, 180k\r\n\tdata/xxxxxxxxx           373     directories containing  10      files, 1.6m\r\n\tdata/admin               11      directories containing  28      files, 56k\r\n\tdata/xxxxxxx             6371    directories containing  0       files, 25m\r\n\tdata/xxxxxxx             3       directories containing  0       files, 12k\r\n\tdata/xxxxxxxxxxx         7       directories containing  1       files, 52k\r\n\tdata/xxxxxx              42      directories containing  0       files, 168k\r\n\r\ni've done some initial investigation with xdebug, with the worst case scenario (viewing the shared top level directory in the web interface, so all the folders shared with me are listed). oc seems to scan all files below it judging from the below calls (there's a large number of files in the count above that aren't in these company-wide shares). here's some of the worst offenders for  total self time:\r\n\r\n\tcall entity\t\t\t\t\t\t\t\t\t\t\tdefined\t\t\t\t\t\t\t\t\t\t\tcalls\t\t\tcumulative total time\t\tself total time\r\n\toc::autoload(...)\t\t\t\t\t\t\t\t\tlib/base.php:80\t\t\t\t\t\t\t\t\t1491805\t\t\t88,474,089 us\t\t\t\t84,241,854 us\r\n\t\\oc\\files\\filesystem::normalizepath(...)\t\t\tlib/files/filesystem.php:568\t\t\t\t\t1491714\t\t\t157,869,757 us\t\t\t\t45,090,823 us\r\n\t\\oc\\share::getitems(...)\t\t\t\t\t\t\tlib/public/share.php:653\t\t\t\t\t\t21\t\t\t\t211,937,577 us\t\t\t\t26,333,808 us\r\n\tclass_exists(...)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t1491722\t\t\t109,976,073 us\t\t\t\t20,325,080 us\r\n\toc_share_backend_folder->getchildren(...)\t\t\tapps/files_sharing/lib/share/folder.php:24\t\t136\t\t\t\t26,011,669 us\t\t\t\t12,488,813 us\r\n\tpdostatementwrapper->fetchrow(...)\t\t\t\t\tlib/db.php:1083\t\t\t\t\t\t\t\t\t1493535\t\t\t12,327,327 us\t\t\t\t9,753,283 us\r\n\tpdostatement->fetch(...)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t1493535\t\t\t1,151,692 us\t\t\t\t1,151,692 us\r\n\t\\oc\\share::getitemsharedwithbysource(...)\t\t\tlib/public/share.php:143\t\t\t\t\t\t17\t\t\t\t213,250,503 us\t\t\t\t1,431,313 us\r\n\r\n### steps to reproduce\r\n1. browse to /shared via the web interface\r\n2. wait for a long time, maybe minutes\r\n3. see, it's slow!\r\n\r\n### server configuration\r\noperating system: centos 6.4 x86_64\r\n\r\nweb server: httpd.x86_64 (apache) 2.2.15-26.el6.centos\r\n\r\ndatabase: mariadb-server.x86_64 5.5.30-1\r\n\r\nphp version: 5.3.10 x86_64 (built from source)\r\n\r\nowncloud version: 5.0.x (currently 5.0.6, but has occured on 5.0.4+ at least, we didn't try the earlier 5.x builds)\r\n\r\n### client configuration\r\nbrowser: firefox, chrome, safari, various versions of each\r\n\r\noperating system: osx, windows, ubuntu, centos\r\n\r\n### logs\r\n#### \r\n\r\nhere are some screenshots from the xdebug cachegrind output. i used xcallgraph to view, wincachegrind crashed out of memory (~50mb compressed, 3gb file when uncompressed). it took several hours on an 8-core machine for xcallgraph to finish loading the file d:\r\n\r\n![callgraph-1](https://f.cloud.github.com/assets/1672327/552025/0b2283f6-c33d-11e2-9a93-f288960c15f8.png)\r\n![callgraph-4](https://f.cloud.github.com/assets/1672327/552027/1f0a623a-c33d-11e2-852c-91da8ca31c79.png)\r\n\r\n","closed_by":{"login":"mtgap","id":1097106,"avatar_url":"https://gravatar.com/avatar/454a73ee5feeb428d0c163a48903332c?d=https%3a%2f%2fidenticons.github.com%2f52d29a66f285a1d61125dd69ac0766b2.png&r=x","gravatar_id":"454a73ee5feeb428d0c163a48903332c","url":"https://api.github.com/users/mtgap","html_url":"https://github.com/mtgap","followers_url":"https://api.github.com/users/mtgap/followers","following_url":"https://api.github.com/users/mtgap/following{/other_user}","gists_url":"https://api.github.com/users/mtgap/gists{/gist_id}","starred_url":"https://api.github.com/users/mtgap/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mtgap/subscriptions","organizations_url":"https://api.github.com/users/mtgap/orgs","repos_url":"https://api.github.com/users/mtgap/repos","events_url":"https://api.github.com/users/mtgap/events{/privacy}","received_events_url":"https://api.github.com/users/mtgap/received_events","type":"user","site_admin":false}}